{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "authorship_tag": "ABX9TyP2vi1n5cHcBj7lqLyzxIY2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/imjongho/AndroidStudio/blob/main/Q%26A_ChatBot2_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nEER2kRiIwvS"
      },
      "outputs": [],
      "source": [
        "# 인공지능 PDF Q&A 챗봇 프로젝트"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "s1WwBRU_Nq9y",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d79e295-5b87-43c8-cd66-eaa544a32aef"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install python-dotenv\n",
        "# 일단 종호의 openai API 키를 .env 파일에 넣어 놓음 => 나중에 바꾸기"
      ],
      "metadata": {
        "collapsed": true,
        "id": "5KCM5kP6No8S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain_openai==0.3.7"
      ],
      "metadata": {
        "collapsed": true,
        "id": "3R-NCn3dOPWk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain-huggingface==0.1.2"
      ],
      "metadata": {
        "collapsed": true,
        "id": "-Mw_Wf-zPhtF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain_community==0.3.18"
      ],
      "metadata": {
        "collapsed": true,
        "id": "hF1lOXXOPkQX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install faiss-cpu==1.10.0"
      ],
      "metadata": {
        "collapsed": true,
        "id": "DW2y0rt0Rjjv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pypdf"
      ],
      "metadata": {
        "collapsed": true,
        "id": "D5quX3Ac8eVU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install pydantic==2.10.6\n",
        "#!pip uninstall -y gradio\n",
        "#!pip install --upgrade gradio gradio-client"
      ],
      "metadata": {
        "collapsed": true,
        "id": "RG_3magVch41"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip uninstall numpy -y\n",
        "#!pip install --no-cache-dir numpy==1.26.4\n",
        "# colab에 numpy 2.버전이 설치되어 있어서 버전 충돌남"
      ],
      "metadata": {
        "collapsed": true,
        "id": "qXZuuZ05P3qO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy\n",
        "print(numpy.__version__)"
      ],
      "metadata": {
        "id": "SqCQ32ozSclI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef4fe850-c2dd-4bc3-e951-104dc9afe28a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.26.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio\n",
        "print(gradio.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ObqvRExvFmXG",
        "outputId": "f17b1718-3ec8-424a-99d9-825443daf0a0"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5.29.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_text_splitters import CharacterTextSplitter\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain_core.chat_history import BaseChatMessageHistory, InMemoryChatMessageHistory\n",
        "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
        "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage, trim_messages\n",
        "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
        "\n",
        "# 환경 변수 불러오기(openai API 키)\n",
        "load_dotenv('/content/drive/MyDrive/Colab Notebooks/.env')\n",
        "\n",
        "# LLM 설정\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
        "\n",
        "# 텍스트 분리\n",
        "text_splitter = CharacterTextSplitter(\n",
        "    chunk_size=500,\n",
        "    chunk_overlap=100\n",
        ")\n",
        "\n",
        "# 임베딩 모델\n",
        "hf_embeddings = HuggingFaceEmbeddings(model_name=\"BAAI/bge-m3\")\n",
        "\n",
        "# 프롬프트 템플릿\n",
        "system_message = \"\"\"\n",
        "당신은 사용자의 질문에 답변을 하는 친절한 AI 어시스턴트입니다.\n",
        "당신의 임무는 주어진 문맥을 토대로 사용자 질문에 답하는 것입니다.\n",
        "만약, 문맥에서 답변을 위한 정보를 찾을 수 없다면 '주어진 정보에서 질문에 대한 정보를 찾을 수 없습니다' 라고 답하세요.\n",
        "정보를 찾을 수 있다면 한글로 답변해 주세요.\n",
        "\"\"\"\n",
        "\n",
        "human_message = \"\"\"\n",
        "## 과거 대화 내역:\n",
        "{memory}\n",
        "\n",
        "## 검색된 문서:\n",
        "{context}\n",
        "\n",
        "## 최신 사용자 질문:\n",
        "{input}\n",
        "\"\"\"\n",
        "prompt_template = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"human\", human_message)\n",
        "    ]\n",
        ")\n",
        "\n",
        "# 출력 파서\n",
        "parser = StrOutputParser()\n",
        "\n",
        "# 트리머 설정\n",
        "trimmer = trim_messages(\n",
        "    max_tokens=200,\n",
        "    token_counter=llm,\n",
        "    strategy=\"last\",\n",
        "    include_system=False,\n",
        "    start_on=\"human\"\n",
        ")\n",
        "\n",
        "# 전역 변수\n",
        "db = None\n",
        "retriever = None\n",
        "rag_chain = None\n",
        "faiss_path = \"/content/drive/MyDrive/FaissDB/knu_faiss_db\"\n",
        "history_store: dict[str, InMemoryChatMessageHistory] = {}\n",
        "session_counter = 1\n",
        "\n",
        "# 세션 초기화 함수\n",
        "def init_session(session_id: str):\n",
        "    if session_id in history_store:\n",
        "        return\n",
        "    messages = InMemoryChatMessageHistory()\n",
        "    messages.add_message(SystemMessage(content=system_message))\n",
        "    history_store[session_id] = messages\n",
        "\n",
        "# 세션 히스토리 함수, 최근 3쌍의 Q&A 메시지만 남기고 나머지 삭제(속도 증가)\n",
        "def get_session_history(session_id: str, limit: int = 6) -> BaseChatMessageHistory:\n",
        "    messages = history_store[session_id]\n",
        "    while len(messages.messages) > limit + 1:\n",
        "        del messages.messages[1]\n",
        "    return messages\n",
        "\n",
        "# RAG 체인 로드\n",
        "def load_chain():\n",
        "    global retriever, rag_chain\n",
        "\n",
        "    # 검색기 context\n",
        "    base_retriever = db.as_retriever(\n",
        "        search_type=\"mmr\",\n",
        "        search_kwargs={\"k\": 3, \"fetch_k\": 10, \"lambda_mult\": 0.5}\n",
        "    )\n",
        "    # LLM을 이용해 여러 개의 서브 쿼리를 만들어 base_retriever에 넘겨 더 풍부한 결과를 가져옴(기본: 3)\n",
        "    retriever = MultiQueryRetriever.from_llm(base_retriever, llm = llm)\n",
        "\n",
        "    # 대화 내역 불러와서 trimmer 적용\n",
        "    history = RunnableWithMessageHistory(RunnablePassthrough(), get_session_history)\n",
        "    trimmed = history | trimmer\n",
        "\n",
        "    # 최종 RAG 체인\n",
        "    rag_chain = {\n",
        "        \"memory\" : trimmed,\n",
        "        \"context\": retriever,\n",
        "        \"input\": RunnablePassthrough()\n",
        "    } | prompt_template | llm | parser\n",
        "\n",
        "# FAISS DB 로드\n",
        "def load_faiss_db():\n",
        "    global db\n",
        "    db = FAISS.load_local(\n",
        "        folder_path=faiss_path,\n",
        "        embeddings=hf_embeddings,\n",
        "        allow_dangerous_deserialization=True\n",
        "    )\n",
        "\n",
        "# PDF 업로드 및 DB 저장\n",
        "def add_pdf_to_db(file):\n",
        "    global db\n",
        "\n",
        "    loader = PyPDFLoader(file.name)\n",
        "    docs = loader.load_and_split(text_splitter=text_splitter)\n",
        "\n",
        "    # 각 청크에 파일명 metadata 추가\n",
        "    for doc in docs:\n",
        "        doc.metadata[\"file_name\"] = os.path.basename(file.name)\n",
        "        # 메타데이터 추가 가능\n",
        "\n",
        "    if db is None:\n",
        "        db = FAISS.from_documents(docs, hf_embeddings)\n",
        "    else:\n",
        "      db.add_documents(docs)\n",
        "\n",
        "    db.save_local(faiss_path)\n",
        "    load_chain()    # DB가 바뀌었으니 retriever/chain을 업데이트\n",
        "    return f\"{os.path.basename(file.name)} 문서를 처리하여 FAISS DB에 저장했습니다.\"\n",
        "\n",
        "# 질문 처리\n",
        "def answer_question(question: str, session_id: str) -> str:\n",
        "    init_session(session_id)\n",
        "    # RAG 체인 호출 → 답변 생성(자동으로 history에 기록됨)\n",
        "    answer = rag_chain.invoke(\n",
        "        question,\n",
        "        config={\"configurable\": {\"session_id\": session_id}}\n",
        "    )\n",
        "    # AI 메시지 수동 저장\n",
        "    get_session_history(session_id).add_message(AIMessage(content=answer))\n",
        "    return answer\n",
        "\n",
        "# FAISS DB에 저장된 문서 목록 보여주는 함수\n",
        "def show_stored_documents():\n",
        "    if db is None:\n",
        "        return \"DB 로드 문제\"\n",
        "    docs = list(db.docstore._dict.values())  # 저장된 모든 청크들을 가져와 리스트로 변환\n",
        "    file_names = {doc.metadata.get(\"file_name\", \"Unknown\") for doc in docs}\n",
        "    return \"📚 저장된 문서 목록:\\n\" + \"\\n\".join(f\"• {f}\" for f in sorted(file_names))\n",
        "\n",
        "# 세션별로 저장된 대화 내역 보여주는 함수(Human/AI)\n",
        "def show_history(session_id: str) -> str:\n",
        "    init_session(session_id)\n",
        "    msgs = get_session_history(session_id).messages\n",
        "\n",
        "    lines = []\n",
        "    if msgs and msgs[0].type == \"system\":\n",
        "        lines.append(f\"System: {msgs[0].content.strip()}\")\n",
        "\n",
        "    seq = [(m.type, m.content.strip()) for m in msgs if m.type in (\"human\", \"ai\")]\n",
        "    pairs = [\n",
        "        f\"Human: {usr}\\nAI: {ai}\"\n",
        "        for (t1, usr), (t2, ai) in zip(seq, seq[1:])\n",
        "        if t1 == \"human\" and t2 == \"ai\"\n",
        "    ]\n",
        "    lines.extend(pairs)\n",
        "    return \"\\n\\n\".join(lines)\n",
        "\n",
        "# Gradio UI 설정\n",
        "with gr.Blocks(theme=gr.themes.Soft()) as demo:\n",
        "    # ── 사이드바: 세션 관리 ──\n",
        "    with gr.Sidebar():\n",
        "        session_dropdown = gr.Dropdown(\n",
        "            label=\"채팅 세션 선택\",\n",
        "            choices=[],\n",
        "            value=None,\n",
        "            interactive=True\n",
        "        )\n",
        "        new_session_btn = gr.Button(\"➕ 새 채팅\")\n",
        "        del_session_btn = gr.Button(\"🗑 세션 삭제\")\n",
        "\n",
        "    gr.Markdown(\"\"\"\n",
        "    # 📄 인공지능 PDF Q&A 챗봇\n",
        "    **여러 PDF 파일을 업로드하고 질문을 입력하면 AI가 답변을 제공합니다!**\n",
        "    \"\"\")\n",
        "\n",
        "    with gr.Row():\n",
        "        with gr.Column(scale=1):\n",
        "            file_input = gr.File(label=\"PDF 파일 선택\")\n",
        "            upload_button = gr.Button(\"📤 벡터 DB에 저장\")\n",
        "            show_files_button = gr.Button(\"📚 저장된 문서 보기\")\n",
        "            status_output = gr.Textbox(label=\"📢 상태 메시지\")\n",
        "\n",
        "        with gr.Column(scale=2):\n",
        "            question_input = gr.Textbox(label=\"❓ 질문 입력\", placeholder=\"궁금한 내용을 적어주세요.\")\n",
        "            submit_button = gr.Button(\"🤖 답변 받기\")\n",
        "            answer_output = gr.Textbox(label=\"📝 AI 답변\")\n",
        "\n",
        "        with gr.Column(scale=3):\n",
        "            show_history_button = gr.Button(\"🕘 히스토리 보기\")\n",
        "            history_output = gr.Textbox(label=\"🗒 전체 대화 내역 | System/Human/AI Message\", lines=15, interactive=False)\n",
        "\n",
        "    # ── 사이드바 이벤트 바인딩 ──\n",
        "    def create_session():\n",
        "        global session_counter\n",
        "        sid = f\"session_{session_counter}\"\n",
        "        session_counter += 1\n",
        "        init_session(sid)\n",
        "        # Dropdown choices 갱신 & 새로 만든 세션으로 선택\n",
        "        return gr.update(choices=list(history_store.keys()), value=sid)\n",
        "\n",
        "    # 세션 삭제 함수\n",
        "    def delete_session(session_id: str):\n",
        "        history_store.pop(session_id, None)\n",
        "\n",
        "    new_session_btn.click(\n",
        "        fn=create_session,\n",
        "        inputs=None,\n",
        "        outputs=[session_dropdown]\n",
        "    )\n",
        "\n",
        "    def remove_session(sid):\n",
        "        delete_session(sid)\n",
        "        keys = list(history_store.keys())\n",
        "        new_val = keys[0] if keys else None\n",
        "        return gr.update(choices=keys, value=new_val)\n",
        "\n",
        "    del_session_btn.click(\n",
        "        fn=remove_session,\n",
        "        inputs=[session_dropdown],\n",
        "        outputs=[session_dropdown]\n",
        "    )\n",
        "\n",
        "    # ── 파일 업로드 / 질문 / 히스토리 이벤트 ──\n",
        "    upload_button.click(add_pdf_to_db, inputs=file_input, outputs=status_output)\n",
        "    submit_button.click(answer_question, inputs=[question_input, session_dropdown], outputs=answer_output)\n",
        "    show_files_button.click(show_stored_documents, outputs=status_output)\n",
        "    show_history_button.click(show_history, inputs=[session_dropdown], outputs=history_output)\n",
        "\n",
        "# 벡터 DB 로드 후 실행\n",
        "load_faiss_db()\n",
        "init_session(\"session_1\")\n",
        "load_chain()\n",
        "demo.launch(debug=True)"
      ],
      "metadata": {
        "id": "krtuPwiOIwfj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}